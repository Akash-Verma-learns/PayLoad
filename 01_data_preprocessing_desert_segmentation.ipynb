{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off-Road Semantic Scene Segmentation - Data Preprocessing\n",
    "## Track 2: Desert Environment Segmentation with Falcon Dataset\n",
    "\n",
    "This notebook handles data loading, preprocessing, and augmentation for the desert semantic segmentation challenge.\n",
    "\n",
    "**Dataset Classes (10 total):**\n",
    "- Trees\n",
    "- Lush Bushes\n",
    "- Dry Grass\n",
    "- Dry Bushes\n",
    "- Ground Clutter\n",
    "- Flowers\n",
    "- Logs\n",
    "- Rocks\n",
    "- Landscape\n",
    "- Sky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths - UPDATE THESE AFTER DOWNLOADING FROM FALCON\n",
    "DATA_ROOT = './falcon_desert_dataset'  # Root directory containing train, val, test\n",
    "TRAIN_IMAGES = os.path.join(DATA_ROOT, 'train/images')\n",
    "TRAIN_MASKS = os.path.join(DATA_ROOT, 'train/masks')\n",
    "VAL_IMAGES = os.path.join(DATA_ROOT, 'val/images')\n",
    "VAL_MASKS = os.path.join(DATA_ROOT, 'val/masks')\n",
    "TEST_IMAGES = os.path.join(DATA_ROOT, 'test/images')\n",
    "\n",
    "# Image preprocessing parameters\n",
    "HEIGHT = 512  # Can adjust based on GPU memory\n",
    "WIDTH = 512\n",
    "NUM_CLASSES = 10  # Desert scene classes\n",
    "\n",
    "# Class names and colors for visualization\n",
    "CLASS_NAMES = [\n",
    "    'Trees',\n",
    "    'Lush Bushes',\n",
    "    'Dry Grass',\n",
    "    'Dry Bushes',\n",
    "    'Ground Clutter',\n",
    "    'Flowers',\n",
    "    'Logs',\n",
    "    'Rocks',\n",
    "    'Landscape',\n",
    "    'Sky'\n",
    "]\n",
    "\n",
    "# Color palette for visualization (RGB)\n",
    "CLASS_COLORS = [\n",
    "    [0, 128, 0],      # Trees - Green\n",
    "    [34, 139, 34],    # Lush Bushes - Forest Green\n",
    "    [189, 183, 107],  # Dry Grass - Dark Khaki\n",
    "    [160, 82, 45],    # Dry Bushes - Sienna\n",
    "    [139, 69, 19],    # Ground Clutter - Saddle Brown\n",
    "    [255, 182, 193],  # Flowers - Light Pink\n",
    "    [101, 67, 33],    # Logs - Dark Brown\n",
    "    [128, 128, 128],  # Rocks - Gray\n",
    "    [210, 180, 140],  # Landscape - Tan\n",
    "    [135, 206, 235]   # Sky - Sky Blue\n",
    "]\n",
    "\n",
    "print(f\"Dataset Configuration:\")\n",
    "print(f\"  Image Size: {HEIGHT}x{WIDTH}\")\n",
    "print(f\"  Number of Classes: {NUM_CLASSES}\")\n",
    "print(f\"  Classes: {', '.join(CLASS_NAMES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_dataset(data_root):\n",
    "    \"\"\"Explore the dataset structure and count images\"\"\"\n",
    "    \n",
    "    splits = ['train', 'val', 'test']\n",
    "    stats = {}\n",
    "    \n",
    "    for split in splits:\n",
    "        img_dir = os.path.join(data_root, split, 'images')\n",
    "        mask_dir = os.path.join(data_root, split, 'masks')\n",
    "        \n",
    "        if os.path.exists(img_dir):\n",
    "            images = sorted(glob.glob(os.path.join(img_dir, '*')))\n",
    "            masks = sorted(glob.glob(os.path.join(mask_dir, '*'))) if os.path.exists(mask_dir) else []\n",
    "            \n",
    "            stats[split] = {\n",
    "                'num_images': len(images),\n",
    "                'num_masks': len(masks),\n",
    "                'has_masks': len(masks) > 0\n",
    "            }\n",
    "            \n",
    "            print(f\"{split.upper()} Set:\")\n",
    "            print(f\"  Images: {len(images)}\")\n",
    "            print(f\"  Masks: {len(masks)}\")\n",
    "            \n",
    "            # Check first image dimensions\n",
    "            if len(images) > 0:\n",
    "                sample_img = Image.open(images[0])\n",
    "                print(f\"  Sample image size: {sample_img.size}\")\n",
    "            print()\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Explore the dataset\n",
    "if os.path.exists(DATA_ROOT):\n",
    "    dataset_stats = explore_dataset(DATA_ROOT)\n",
    "else:\n",
    "    print(f\"⚠️ Dataset directory not found: {DATA_ROOT}\")\n",
    "    print(\"Please download the dataset from Falcon and update DATA_ROOT path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DesertSegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for Desert Semantic Segmentation\n",
    "    \n",
    "    Args:\n",
    "        image_dir: Directory containing RGB images\n",
    "        mask_dir: Directory containing segmentation masks\n",
    "        transform: Transformations to apply\n",
    "        is_train: Whether this is training set (for augmentation)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_dir, mask_dir=None, transform=None, is_train=True):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # Get all image paths\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(image_dir, '*')))\n",
    "        \n",
    "        # Get mask paths if available\n",
    "        if mask_dir is not None and os.path.exists(mask_dir):\n",
    "            self.mask_paths = sorted(glob.glob(os.path.join(mask_dir, '*')))\n",
    "        else:\n",
    "            self.mask_paths = None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Load mask if available\n",
    "        if self.mask_paths is not None:\n",
    "            mask_path = self.mask_paths[idx]\n",
    "            mask = Image.open(mask_path)\n",
    "            \n",
    "            # Convert mask to numpy for easier processing\n",
    "            mask = np.array(mask)\n",
    "            \n",
    "            # If mask is RGB, convert to class indices\n",
    "            if len(mask.shape) == 3:\n",
    "                mask = self._rgb_to_class_idx(mask)\n",
    "        else:\n",
    "            mask = None\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image, mask = self.transform(image, mask)\n",
    "        \n",
    "        if mask is not None:\n",
    "            return image, mask\n",
    "        else:\n",
    "            return image, os.path.basename(img_path)\n",
    "    \n",
    "    def _rgb_to_class_idx(self, mask_rgb):\n",
    "        \"\"\"Convert RGB mask to class indices\"\"\"\n",
    "        h, w = mask_rgb.shape[:2]\n",
    "        mask_idx = np.zeros((h, w), dtype=np.int64)\n",
    "        \n",
    "        for class_idx, color in enumerate(CLASS_COLORS):\n",
    "            matches = np.all(mask_rgb == color, axis=-1)\n",
    "            mask_idx[matches] = class_idx\n",
    "        \n",
    "        return mask_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Augmentation and Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointTransform:\n",
    "    \"\"\"\n",
    "    Apply transformations to both image and mask\n",
    "    Important for segmentation tasks where image and mask must be transformed together\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, height=512, width=512, is_train=True):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # Normalization values (ImageNet stats)\n",
    "        self.normalize = T.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    \n",
    "    def __call__(self, image, mask=None):\n",
    "        # Resize\n",
    "        image = TF.resize(image, (self.height, self.width), \n",
    "                         interpolation=T.InterpolationMode.BILINEAR)\n",
    "        if mask is not None:\n",
    "            mask = cv2.resize(mask, (self.width, self.height), \n",
    "                            interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        # Training augmentations\n",
    "        if self.is_train:\n",
    "            # Random horizontal flip\n",
    "            if np.random.rand() > 0.5:\n",
    "                image = TF.hflip(image)\n",
    "                if mask is not None:\n",
    "                    mask = np.fliplr(mask).copy()\n",
    "            \n",
    "            # Random vertical flip\n",
    "            if np.random.rand() > 0.5:\n",
    "                image = TF.vflip(image)\n",
    "                if mask is not None:\n",
    "                    mask = np.flipud(mask).copy()\n",
    "            \n",
    "            # Random rotation (in 90-degree increments)\n",
    "            if np.random.rand() > 0.75:\n",
    "                angle = np.random.choice([90, 180, 270])\n",
    "                image = TF.rotate(image, angle, interpolation=T.InterpolationMode.BILINEAR)\n",
    "                if mask is not None:\n",
    "                    k = angle // 90\n",
    "                    mask = np.rot90(mask, k).copy()\n",
    "            \n",
    "            # Color jitter (only for image, not mask)\n",
    "            if np.random.rand() > 0.5:\n",
    "                brightness = np.random.uniform(0.8, 1.2)\n",
    "                contrast = np.random.uniform(0.8, 1.2)\n",
    "                saturation = np.random.uniform(0.8, 1.2)\n",
    "                hue = np.random.uniform(-0.1, 0.1)\n",
    "                \n",
    "                image = TF.adjust_brightness(image, brightness)\n",
    "                image = TF.adjust_contrast(image, contrast)\n",
    "                image = TF.adjust_saturation(image, saturation)\n",
    "                image = TF.adjust_hue(image, hue)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        image = TF.to_tensor(image)\n",
    "        image = self.normalize(image)\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = torch.from_numpy(mask).long()\n",
    "            return image, mask\n",
    "        else:\n",
    "            return image, None\n",
    "\n",
    "\n",
    "# Create transform instances\n",
    "train_transform = JointTransform(height=HEIGHT, width=WIDTH, is_train=True)\n",
    "val_transform = JointTransform(height=HEIGHT, width=WIDTH, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample(dataset, idx, figsize=(15, 5)):\n",
    "    \"\"\"Visualize a sample from the dataset\"\"\"\n",
    "    \n",
    "    image, mask = dataset[idx]\n",
    "    \n",
    "    # Denormalize image for visualization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    image_vis = image * std + mean\n",
    "    image_vis = torch.clamp(image_vis, 0, 1)\n",
    "    \n",
    "    # Convert to numpy for plotting\n",
    "    image_np = image_vis.permute(1, 2, 0).numpy()\n",
    "    \n",
    "    if isinstance(mask, torch.Tensor):\n",
    "        mask_np = mask.numpy()\n",
    "        \n",
    "        # Create colored mask\n",
    "        mask_color = np.zeros((mask_np.shape[0], mask_np.shape[1], 3), dtype=np.uint8)\n",
    "        for class_idx in range(NUM_CLASSES):\n",
    "            mask_color[mask_np == class_idx] = CLASS_COLORS[class_idx]\n",
    "        \n",
    "        # Plot\n",
    "        fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "        \n",
    "        axes[0].imshow(image_np)\n",
    "        axes[0].set_title('Original Image')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(mask_color)\n",
    "        axes[1].set_title('Ground Truth Mask')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Overlay\n",
    "        overlay = (image_np * 0.6 + mask_color / 255 * 0.4)\n",
    "        axes[2].imshow(overlay)\n",
    "        axes[2].set_title('Overlay')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "    else:\n",
    "        # Test set without masks\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(image_np)\n",
    "        plt.title(f'Test Image: {mask}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_class_distribution(dataset, title=\"Class Distribution\"):\n",
    "    \"\"\"Plot the distribution of classes in the dataset\"\"\"\n",
    "    \n",
    "    class_counts = np.zeros(NUM_CLASSES)\n",
    "    \n",
    "    print(\"Analyzing class distribution...\")\n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        _, mask = dataset[idx]\n",
    "        if isinstance(mask, torch.Tensor):\n",
    "            unique, counts = torch.unique(mask, return_counts=True)\n",
    "            for u, c in zip(unique, counts):\n",
    "                if u < NUM_CLASSES:\n",
    "                    class_counts[u] += c.item()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(range(NUM_CLASSES), class_counts, color=['#{:02x}{:02x}{:02x}'.format(*c) for c in CLASS_COLORS])\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Pixel Count')\n",
    "    plt.title(title)\n",
    "    plt.xticks(range(NUM_CLASSES), CLASS_NAMES, rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    for i, (name, count) in enumerate(zip(CLASS_NAMES, class_counts)):\n",
    "        percentage = (count / class_counts.sum()) * 100\n",
    "        print(f\"  {name:15s}: {count:12.0f} pixels ({percentage:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DATA_ROOT):\n",
    "    # Create datasets\n",
    "    train_dataset = DesertSegmentationDataset(\n",
    "        image_dir=TRAIN_IMAGES,\n",
    "        mask_dir=TRAIN_MASKS,\n",
    "        transform=train_transform,\n",
    "        is_train=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = DesertSegmentationDataset(\n",
    "        image_dir=VAL_IMAGES,\n",
    "        mask_dir=VAL_MASKS,\n",
    "        transform=val_transform,\n",
    "        is_train=False\n",
    "    )\n",
    "    \n",
    "    test_dataset = DesertSegmentationDataset(\n",
    "        image_dir=TEST_IMAGES,\n",
    "        mask_dir=None,  # No masks for test set\n",
    "        transform=val_transform,\n",
    "        is_train=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset sizes:\")\n",
    "    print(f\"  Training: {len(train_dataset)} samples\")\n",
    "    print(f\"  Validation: {len(val_dataset)} samples\")\n",
    "    print(f\"  Test: {len(test_dataset)} samples\")\n",
    "else:\n",
    "    print(\"⚠️ Please download and set up the Falcon dataset first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DATA_ROOT):\n",
    "    # Visualize random training samples\n",
    "    print(\"Training Set Samples:\")\n",
    "    for i in range(3):\n",
    "        idx = np.random.randint(0, len(train_dataset))\n",
    "        visualize_sample(train_dataset, idx)\n",
    "    \n",
    "    # Visualize random validation samples\n",
    "    print(\"\\nValidation Set Samples:\")\n",
    "    for i in range(2):\n",
    "        idx = np.random.randint(0, len(val_dataset))\n",
    "        visualize_sample(val_dataset, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analyze Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DATA_ROOT):\n",
    "    # Analyze training set distribution (sample subset to save time)\n",
    "    sample_size = min(100, len(train_dataset))\n",
    "    sample_indices = np.random.choice(len(train_dataset), sample_size, replace=False)\n",
    "    \n",
    "    class SubsetDataset(Dataset):\n",
    "        def __init__(self, dataset, indices):\n",
    "            self.dataset = dataset\n",
    "            self.indices = indices\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.indices)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            return self.dataset[self.indices[idx]]\n",
    "    \n",
    "    sample_dataset = SubsetDataset(train_dataset, sample_indices)\n",
    "    plot_class_distribution(sample_dataset, f\"Training Set Class Distribution (Sample of {sample_size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Preprocessed Data Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DATA_ROOT):\n",
    "    # Save dataset configuration\n",
    "    config = {\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'class_names': CLASS_NAMES,\n",
    "        'class_colors': CLASS_COLORS,\n",
    "        'image_size': [HEIGHT, WIDTH],\n",
    "        'train_size': len(train_dataset),\n",
    "        'val_size': len(val_dataset),\n",
    "        'test_size': len(test_dataset),\n",
    "    }\n",
    "    \n",
    "    with open('dataset_config.json', 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    print(\"Dataset configuration saved to 'dataset_config.json'\")\n",
    "    print(\"\\nPreprocessing complete! Ready for model training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook completed:\n",
    "1. ✅ Dataset loading and exploration\n",
    "2. ✅ Custom Dataset class for desert segmentation\n",
    "3. ✅ Data augmentation pipeline\n",
    "4. ✅ Visualization utilities\n",
    "5. ✅ Class distribution analysis\n",
    "6. ✅ Configuration saving\n",
    "\n",
    "**Next Steps:**\n",
    "- Proceed to `02_model_train_desert_segmentation.ipynb` for model training\n",
    "- Experiment with different augmentation strategies\n",
    "- Consider class imbalance for loss function weighting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
